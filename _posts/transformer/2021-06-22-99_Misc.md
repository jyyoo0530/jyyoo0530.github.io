###### Tensor

torch.Tensor는 하나의 데이터타입을 소유하는 다차원 매트릭스(multi-dimensional matrix)를 포함하는 요소. floating, complex, integer, boolean등을 사용한다.

###### torch의 연산

- 행렬의 곱셈, 곱셈(원소별) - `Tensor.matmul()`일반적인 행렬곱셈, `matmul()`동일한 사이즈의 행렬에 대해 동일한 위치의 원소끼리 곱함
- 평균 - `Tensor.mean(dim=n)` 평균을 구하되 dim이 주어졌을 경우 해당 dim에 해당하는 차원의 평균을 계산함(해당차원은 소멸되므로 `원래 차원(dim) - 1` 이 새로운 차원이 됨.)
- 합 - `Tensor.sum(dim=n)` 합이며 위와 같음.
- 최대와 최대값 - `Tensor.max()`는 텐서의 최대값을 리턴하고, `Tensor.argmax()`는 최대값을 가지는 인덱스를 리턴함.
- `Tensor.view(array)` - 해당 텐서를 array의 차원으로 변환한다(array의 원소들을 위한 사이즈와 원래 사이즈는 같아야함.)
- `Tensor.squeeze(n)`, `Tensor.unsqueeze(n)` - n위치의 1차원을 추가/삭제한다.
- `Tensor.cat([],dim=n)` - n번째 차원에 []를 붙인다.
- `Tensor.{xxx}_` - xxx연산자에다가 `_`를 붙이면(eg, Tensor.mul_(something)) 결과 값을 원래 값에 덮어쓴다. 


###### Layer Normalization

https://arxiv.org/abs/1607.06450
- 정규화의 목표는 값 범위의 차이를 왜곡시키지 않고 데이터 세트를 공통된 scale로 변경하는 것.(Min-Max Scalar같이 데이터를 0~1사이로 조정하는 방법을 많이 씀)
- Regularization(일반화,정규화)라는 용어도 있는데, 일종의 "제약"을 주는 것(weight decay, dropout, pruning등)
- Standardization(표준화)를 뜻하며, standard scaler 또는 z-score normalization을 의미. 기존 데이터를 평균이 0, 표준편차 1인 표준분포의 데이터로 전환하는 것.

ICS(Internal Covariate Shift)란 - 네트워크 각 층이나 activation마다 입력값의 distribution이 달라지는 현상
Batch Normalization과 Layer Normalization이 있음.

<img src="/resources/transformer/normalization.png" width="600" height="500"><br>

- Batch Normalization

  - tanh나 sigmoid 같은 활성화 함수에 대해 그래디언트 소실(vanishing gradient)문제가 감소한다.
  - 가중치 초기화에 덜 민감하다. 가중치 초기값에 크게 의존하지 않기 때문에 05-1. 심층 신경망 학습에서 알아본 가중치 초기화 기법에 대해 크게 신경 쓰지 않아도 된다.
  - 학습률(learning rate)를 크게 잡아도 gradient descent가 잘 수렴한다.
  - 오버피팅을 억제한다. BN이 마치 Regularization 역할을 하기 때문에 드롭아웃(Dropout)과 같은 규제기법에 대한 필요성이 감소한다.  하지만, BN로 인한 규제는 효과가 크지 않기 때문에 드롭아웃을 함께 사용하는 것이 좋다.
  - batch 단위(vector내의 element가 아닌, vector간의 같은 위치의 element에 대해 평균을 냄)
  - 다음의 경우에 잘 작동하지 않는다.
    - 개별 데이터가 커서, mini-batch의 샘플 수가 너무 적은 경우
    - RNN같은 sequence 데이터의 경우에 각각의 Time-step마다 BN을 적용시켜야 하는데 이는 엄청난 계산량이 든다.
    - 가장 중요한 것은 각각 time-step마다 다른 BN이 학습된다.

- Layer Normalization 
  - BN과 달리 각층 내부의 정규화를 시행한다. (한 층의 layer들의 hidden variable H에 대해서 정규화 수행)
  - 문자열을 다루는 문제(transformer)와 같이 seq가 중요한 모델은 이 정규화 모델 사용.

#######Reference
http://igl-blog.com/tag/normalization/
https://wingnim.tistory.com/92


###### Text Preprocessing
1) Text Normalization (영어)
 - 모든 대문자 소문자로 변경
 - 숫자를 문자로 변경 또는 제거
 - 마침표, 쉼표 등 특수기호 및 발음기호 제거
 - 빈칸 제거
 - 약자 추가
 - removing stop words, sparse terms, and particular words
 - text canonicalization

####### Reference
https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908

<br>

###### 차원 축소화 알고리즘

서론

축소 방법에는 투영(projection)과 매니폴드(manifold)방법이 있다.
투영은 말그대로 낮은 차원에 투영된 그림자를 보는것, 매니폴드는 스위스롤(swiss roll) 같이 롤케잌을 똑바로 펴는것.(똑바로 편다고 무조건 좋은 건 아님)

1) PCA(Principle Component Analysis, 주성분분석)

  분포된 주 성분을 찾아주는 알고리즘. 어떠한 데이터 분포에서 그 데이터의 특징을 가장 잘 나타내기 위하여 중앙으로부터 가장 먼 방향의 벡터(주성분)를 차원에 맞게 반환해 주는 것.
  a.분산이 최대가 되는 초평면을 구한 뒤, b.첫번째 축과 직교하면서 분산이 최대인 두번째 축을 찾고, c. 차원의 갯수만큼 축들과 직교하면서 분산이 최대인 축들을 찾아나가는 것.
  PCA자체로 차원이 줄지는 않지만, 특성 표시에 중요한 차원 외에는 제거해버리고(투영) 사용하는 것으로 차원이 축소가 된다.
  여기에는 데이터가 뭉개져서 특성을 제대로 표시하지 못하게 될 가능성이 있다는 한계점이 있다.


2) t-SNE(t-distributed Stochastic Neighbor Embedding)
   PCA에서 제기된 한계점(중첩으로 인한 특성 소실)을 극복하기 위해, 한 데이터에서 다른 데이터들 끼리의 거리를 측정하여 이를 t-distribution에 놓고 비슷한 묶음 끼리 묶은 것이다.
   데이터 중첩없이 유사도를 투영할 수 있지만, 데이터의 수에따라 O^n번 계산을 하여야 한다는(계산량증가)문제가 있다.


####### Reference
https://ko.wikipedia.org/wiki/%EC%A3%BC%EC%84%B1%EB%B6%84_%EB%B6%84%EC%84%9D
https://darkpgmr.tistory.com/110
https://excelsior-cjh.tistory.com/168?category=918734
