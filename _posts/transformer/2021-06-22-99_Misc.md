###### Tensor

torch.Tensor는 하나의 데이터타입을 소유하는 다차원 매트릭스(multi-dimensional matrix)를 포함하는 요소. floating, complex, integer, boolean등을 사용한다.

###### torch의 연산

- 행렬의 곱셈, 곱셈(원소별) - `Tensor.matmul()`일반적인 행렬곱셈, `matmul()`동일한 사이즈의 행렬에 대해 동일한 위치의 원소끼리 곱함
- 평균 - `Tensor.mean(dim=n)` 평균을 구하되 dim이 주어졌을 경우 해당 dim에 해당하는 차원의 평균을 계산함(해당차원은 소멸되므로 `원래 차원(dim) - 1` 이 새로운 차원이 됨.)
- 합 - `Tensor.sum(dim=n)` 합이며 위와 같음.
- 최대와 최대값 - `Tensor.max()`는 텐서의 최대값을 리턴하고, `Tensor.argmax()`는 최대값을 가지는 인덱스를 리턴함.
- `Tensor.view(array)` - 해당 텐서를 array의 차원으로 변환한다(array의 원소들을 위한 사이즈와 원래 사이즈는 같아야함.)
- `Tensor.squeeze(n)`, `Tensor.unsqueeze(n)` - n위치의 1차원을 추가/삭제한다.
- `Tensor.cat([],dim=n)` - n번째 차원에 []를 붙인다.
- `Tensor.{xxx}_` - xxx연산자에다가 `_`를 붙이면(eg, Tensor.mul_(something)) 결과 값을 원래 값에 덮어쓴다.

###### Layer Normalization

https://arxiv.org/abs/1607.06450

- 정규화의 목표는 값 범위의 차이를 왜곡시키지 않고 데이터 세트를 공통된 scale로 변경하는 것.(Min-Max Scalar같이 데이터를 0~1사이로 조정하는 방법을 많이 씀)
- Regularization(일반화,정규화)라는 용어도 있는데, 일종의 "제약"을 주는 것(weight decay, dropout, pruning등)
- Standardization(표준화)를 뜻하며, standard scaler 또는 z-score normalization을 의미. 기존 데이터를 평균이 0, 표준편차 1인 표준분포의 데이터로 전환하는 것.

ICS(Internal Covariate Shift)란 - 네트워크 각 층이나 activation마다 입력값의 distribution이 달라지는 현상 Batch Normalization과 Layer
Normalization이 있음.

<img src="/resources/transformer/normalization.png" width="600" height="500"><br>

- Batch Normalization

  - tanh나 sigmoid 같은 활성화 함수에 대해 그래디언트 소실(vanishing gradient)문제가 감소한다.
  - 가중치 초기화에 덜 민감하다. 가중치 초기값에 크게 의존하지 않기 때문에 05-1. 심층 신경망 학습에서 알아본 가중치 초기화 기법에 대해 크게 신경 쓰지 않아도 된다.
  - 학습률(learning rate)를 크게 잡아도 gradient descent가 잘 수렴한다.
  - 오버피팅을 억제한다. BN이 마치 Regularization 역할을 하기 때문에 드롭아웃(Dropout)과 같은 규제기법에 대한 필요성이 감소한다. 하지만, BN로 인한 규제는 효과가 크지 않기 때문에
    드롭아웃을 함께 사용하는 것이 좋다.
  - batch 단위(vector내의 element가 아닌, vector간의 같은 위치의 element에 대해 평균을 냄)
  - 다음의 경우에 잘 작동하지 않는다.
    - 개별 데이터가 커서, mini-batch의 샘플 수가 너무 적은 경우
    - RNN같은 sequence 데이터의 경우에 각각의 Time-step마다 BN을 적용시켜야 하는데 이는 엄청난 계산량이 든다.
    - 가장 중요한 것은 각각 time-step마다 다른 BN이 학습된다.

- Layer Normalization
  - BN과 달리 각층 내부의 정규화를 시행한다. (한 층의 layer들의 hidden variable H에 대해서 정규화 수행)
  - 문자열을 다루는 문제(transformer)와 같이 seq가 중요한 모델은 이 정규화 모델 사용.

###### #Reference

http://igl-blog.com/tag/normalization/
https://wingnim.tistory.com/92

###### Text Preprocessing

1) Text Normalization (영어)

- 모든 대문자 소문자로 변경
- 숫자를 문자로 변경 또는 제거
- 마침표, 쉼표 등 특수기호 및 발음기호 제거
- 빈칸 제거
- 약자 추가
- removing stop words, sparse terms, and particular words
- text canonicalization

###### # Reference

https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908

<br>

###### 차원 축소화 알고리즘

서론

축소 방법에는 투영(projection)과 매니폴드(manifold)방법이 있다. 투영은 말그대로 낮은 차원에 투영된 그림자를 보는것, 매니폴드는 스위스롤(swiss roll) 같이 롤케잌을 똑바로 펴는것.(똑바로
편다고 무조건 좋은 건 아님)

1) PCA(Principle Component Analysis, 주성분분석)

분포된 주 성분을 찾아주는 알고리즘. 어떠한 데이터 분포에서 그 데이터의 특징을 가장 잘 나타내기 위하여 중앙으로부터 가장 먼 방향의 벡터(주성분)를 차원에 맞게 반환해 주는 것. a.분산이 최대가 되는 초평면을
구한 뒤, b.첫번째 축과 직교하면서 분산이 최대인 두번째 축을 찾고, c. 차원의 갯수만큼 축들과 직교하면서 분산이 최대인 축들을 찾아나가는 것. PCA자체로 차원이 줄지는 않지만, 특성 표시에 중요한 차원 외에는
제거해버리고(투영) 사용하는 것으로 차원이 축소가 된다. 여기에는 데이터가 뭉개져서 특성을 제대로 표시하지 못하게 될 가능성이 있다는 한계점이 있다.

2) t-SNE(t-distributed Stochastic Neighbor Embedding)
   PCA에서 제기된 한계점(중첩으로 인한 특성 소실)을 극복하기 위해, 한 데이터에서 다른 데이터들 끼리의 거리를 측정하여 이를 t-distribution에 놓고 비슷한 묶음 끼리 묶은 것이다. 데이터 중첩없이
   유사도를 투영할 수 있지만, 데이터의 수에따라 O^n번 계산을 하여야 한다는(계산량증가)문제가 있다.
   `sklearn.manifold`의 `TSNE`에서 다음의 변수들이 컨트롤 가능
  - n_components: 압축할 차원의 수
  - perplexity: 알고리즘에 영향을 줄 주변 값들의 크기 (보통 5~50사이로 함, 데이터의 크기가 클수록 큰 값이 필요, 기본값 30)
  - early_exaggeration: 데이터의 응집력에 영향을 주는 요소(데이터의 크기에 정비례하는것이 좋으며, 코스트펑션의 초기값이 크다면, 이 값이 너무 높다는 것을 의미, 기본값 12)
  - learning_rate:보통 10~1000사이로 함. 너무 높으면 모든 포인트간의 거리가 동일한 큰 "공"처럼 보이고, 너무 낮으면 몇개의 아웃라이어 외에는 상당히 응집되어 보임. 기본값 200
  - n_iter: 초기화를 위한 최대반복 횟수, 최소 250이어야함. 초기값 1000
  - n_iter_without_progress: 최적화 확인 없이 진행할 반복횟수, 배 50번반복마다 확인하므로 50의 배수여야함. 초기값 300
  - min_grad_norm: gradient norm이 이 값보다 낮아지면, 최적화를 중지함. 기본값 1e-7
  - metric: 값들의 거리에 사용될 공간계. 기본값 'euclidean'
  - init:
  - verbose: 상세도 셋팅, 기본값 0
  - random_state: 비용함수의 로컬미니마에 영향을 주는 요소.
  - method: 경사하강계산 알고리즘, 기본값 'barnes_hut'
  - angle:
  - n_job:
  - square_distances:

###### # Reference

https://ko.wikipedia.org/wiki/%EC%A3%BC%EC%84%B1%EB%B6%84_%EB%B6%84%EC%84%9D
https://darkpgmr.tistory.com/110
https://excelsior-cjh.tistory.com/168?category=918734
https://lvdmaaten.github.io/tsne/

###### Gradient Descent(경사하강)

함수의 최소값을 찾는 문제. Xi+1 = Xi - a미분f(Xi)

- 이슈
  - step size(a)가 적절해야함. 너무 작으면 도달하지못하고, 너무 크면 최저점을 지나버려 발산해버림
  - local minima문제. minimum 포인트가 여러개 존재하는 경우, 시작점을 잘못 짚으면 global minimum이 아니라 local minimum에서 멈춰버리는 이슈가 발생.
  - 이를 응용한 알고리즘들: SGD(Stochastic Gradient Recent), Momentum, NAG(Nesterov Accelerated Gradient), Adagard, Adadelta,
    RMSProp

###### # Reference

https://angeloyeo.github.io/2020/08/16/gradient_descent.html
http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html

###### Loss Function(손실함수)

예측값과 실제값의 차이를 구하는 함수 회귀에서 사용되는 손실함수의 종류는 MAE, MSE, RMSE등이 있고, 분류에서 사용되는 종류는 Binary Cross-Entropy, Categorical
Cross-Entropy등이 있음.

###### Reference

https://brunch.co.kr/@mnc/9

###### Pytorch에서의 전처리 이해

토큰화 -> 단어집합(build_vocab)생성, 키:단어, 밸류:빈도수 토치텍스트의 제공 기능:

- 파일로드: 다양한 포맷의 코퍼스 로딩
- 토큰화: 문장을 단어 단위로 분리
- 단어집합: 단어 집합 만들기
- 정수 인코딩: 전체 코퍼스의 단어들을 고유한 정수로 매핑
- 단어벡터: 단어 집합에 대해 고유한 임베딩 벡터 생성. 랜덤값 or 사전훈련된 임베딩 벡터
- 배치화: 훈련샘플들의 배치 생성

###### Dropout이란?

Overfitting을 해결하기 위한 솔루션

- 학습셋을 늘리거나
- Feature의 수를 줄이거나
- Regularization Term을 추가하거나
- Dropout을 하는 방법들이 있다.

(a) 기본적인 인공신경망(standard neural net) ouput과 gradient loss 간의 차이(=loss)를 구한 뒤, backpropagation 알고리즘을 구하여 weight를 업데이트하는
방식이다.

(b) 뉴런이라고 부르는 노드를 무작위로 껐다 켰다를 반복하는 것을 dropout이라고 한다. 예를 들어, 학습 데이터 x가 주어졌을 때, 각 레이어별로 x 데이터의 사용 여부를 결정하는 것이다. (b) 그래프에서
첫 번째 레이어를 보면 2, 3번째 노드 x를 사용하지 않은 것을 볼 수 있다. 두 번쩨 레이어에서는 1, 3,4번째 노드는 사용하지 않았다. 최종적으로 세 개의 weight만 이용하여 계산하게 되고, ouput과
G(t) 간의 차이를 이용해 loss를 구한 뒤, backpropagation 하여 값들을 업데이트한다.

효과

- overfitting방지
- 성능을 향상
- 앙상블 효과

아래는 논문에서 발췌한 내용 키 아이디어는 NN의 훈련 도중에 랜덤하게 유닛을 포기하는 것.

Backpropagation

###### # Reference

https://wegonnamakeit.tistory.com/46
https://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf

###### Activation Function(활성화 함수)?

딥러닝 네트워크에서 노드에 입력된 값들을 비선형 함수에 통과시킨 후 다음레이어로 전달하는데, 이 때 사용하는 함수를 활성화함수라고함. 입력받은 값을 활성화를 하느냐 비활성화를 하느냐를 결정하는 함수

종류

- Sigmoid 함수(Logistic 함수)
  Vanishing Gradient Problem이 있음. (미분함수에 대해 일정 값 이상에서 미분값이 소실되는 문제)
- Tanh(Hyperbolic Tangent Function) 함수 Sigmoid 함수의 중심점을 0으로 옮기고 값을 -1 ~ 1 사이로 오도록 수식 조정한 것.
- ReLU(Rectified Linear Unit, 경사함수)
  가장 많이 사용되는 함수 중 하나.
- 이 외에, Leaky ReLU, PReLU, ELU, Maxout

###### # Reference

https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=handuelly&logNo=221824080339

###### Feedforward Neural Networks

레이어간 피드백 통신없이 Output방향으로 바로 통신하는 신경망 구조.

###### # Reference

http://www.cs.cmu.edu/~10701/slides/Perceptron_Reading_Material.pdf

###### Perceptron

다수의 신호를 입력받아 하나의 신호, 출력한다/안한다(1/0), 로 출력하는것.

###### # Reference

https://excelsior-cjh.tistory.com/169

###### 가중치 초기화

학습시킬때의 가중치를 초기화 시키는 것, 초기 가중치가 0이거나 동일할 경우, 각 뉴런에서 동일한 출력값을 보내 역전파 단계에서 각 뉴런이 모두 동일한 그래디언트 값을 지니게 된다. 관건은, 가중치 초기값을 작게
초기화 해야하며 동일한 초기값을 가지지 않도록 랜던함게 초기화 해야한다. 일반적으로 초기값 평균이 0이고 표준편차가 0.01인 정규분포(가우시안분포)를 사용하면 될 것 같지만, 역시 안됨.

- Xavier초기화


- He 초기화

###### # Reference

https://excelsior-cjh.tistory.com/177?category=940400

###### 딥러닝 모델의 평가

1. 정확도(Accuracy)

- 가장 직관적으로 사용 가능
- 정확도 = 예측결과가 동일한 데이터 / 전체 예측 데이터

2. 오차행렬(Confusion Matrix)

- 정확도 = TP+TN/TP+TN+FP+FN

3. 정밀도(Precision) 및 재현율(Recall)

- 정밀도 = TP/(FP+TP)
- 재현율(Recall, TPR(True Positive Rate)) = TP/(FN+TP)

4. F1 스코어(Score)

- F1 스코어는 정밀도와 재현율을 결합한 지표.(조화평균을 낸 것)
- F1 Score = 2 / ((1/recall) + (1/precision))   

5. ROC(Receiver Operating Characteristic) Curve와 AUC(Area Under Curve) Score 
- ROC Curve = (FPR(X), TPR(Y))
- FPR(False Positive Rate) = FP/(FP+TN) = 1-TNR = 1 - 특이성
- TPR(True Positive Rate, 재현율)
- 현의 휨정도 = 위로 클수록 Classifier가 TP/FP를 잘 구분한다는 의미
- 현 위의 위치한 점 = T/F를 판단하는 threshold가 변경된다는 의미(분류기의 분류 성능은 그대로)

####### Reference
https://john-analyst.medium.com/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EB%AA%A8%EB%8D%B8%EC%9D%98-%ED%8F%89%EA%B0%80-9f6686edaac


###### Learning Rate
Gradient의 경우에, 학습률(learning rate)또는 보폭(step size)라 불리는 스칼라를 곱해 다음 지점을 결정. 너무 크지도 작지도 않은 값을 설정해야함.
학습률이 너무 크면, 발산하여 최저점으로부터 벗어나게 되고, 학습률이 너무 작으면, 학습시간이 오래걸리며 최저점에 도달하지 못함.


####### Reference
https://bioinformaticsandme.tistory.com/130


###### torch.autograd의 backward()에 대해
backward 그래프는 forward 작동시 이미 생성되어 있고, backward()를 작동하게 되면 이미 생성된 backward graph를 사용해 gradient를 계산하고
이를 leaf node에 저장하게됨.

####### Reference
https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95
