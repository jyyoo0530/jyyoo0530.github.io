###### Tensor

torch.Tensor는 하나의 데이터타입을 소유하는 다차원 매트릭스(multi-dimensional matrix)를 포함하는 요소. floating, complex, integer, boolean등을 사용한다.

###### torch의 연산

- 행렬의 곱셈, 곱셈(원소별) - `Tensor.matmul()`일반적인 행렬곱셈, `matmul()`동일한 사이즈의 행렬에 대해 동일한 위치의 원소끼리 곱함
- 평균 - `Tensor.mean(dim=n)` 평균을 구하되 dim이 주어졌을 경우 해당 dim에 해당하는 차원의 평균을 계산함(해당차원은 소멸되므로 `원래 차원(dim) - 1` 이 새로운 차원이 됨.)
- 합 - `Tensor.sum(dim=n)` 합이며 위와 같음.
- 최대와 최대값 - `Tensor.max()`는 텐서의 최대값을 리턴하고, `Tensor.argmax()`는 최대값을 가지는 인덱스를 리턴함.
- `Tensor.view(array)` - 해당 텐서를 array의 차원으로 변환한다(array의 원소들을 위한 사이즈와 원래 사이즈는 같아야함.)
- `Tensor.squeeze(n)`, `Tensor.unsqueeze(n)` - n위치의 1차원을 추가/삭제한다.
- `Tensor.cat([],dim=n)` - n번째 차원에 []를 붙인다.
- `Tensor.{xxx}_` - xxx연산자에다가 `_`를 붙이면(eg, Tensor.mul_(something)) 결과 값을 원래 값에 덮어쓴다. 


###### Layer Normalization

https://arxiv.org/abs/1607.06450
- 정규화의 목표는 값 범위의 차이를 왜곡시키지 않고 데이터 세트를 공통된 scale로 변경하는 것.(Min-Max Scalar같이 데이터를 0~1사이로 조정하는 방법을 많이 씀)
- Regularization(일반화,정규화)라는 용어도 있는데, 일종의 "제약"을 주는 것(weight decay, dropout, pruning등)
- Standardization(표준화)를 뜻하며, standard scaler 또는 z-score normalization을 의미. 기존 데이터를 평균이 0, 표준편차 1인 표준분포의 데이터로 전환하는 것.

ICS(Internal Covariate Shift)란 - 네트워크 각 층이나 activation마다 입력값의 distribution이 달라지는 현상
Batch Normalization과 Layer Normalization이 있음.

<img src="/resources/transformer/normalization.png" width="600" height="500"><br>

- Batch Normalization

  - tanh나 sigmoid 같은 활성화 함수에 대해 그래디언트 소실(vanishing gradient)문제가 감소한다.
  - 가중치 초기화에 덜 민감하다. 가중치 초기값에 크게 의존하지 않기 때문에 05-1. 심층 신경망 학습에서 알아본 가중치 초기화 기법에 대해 크게 신경 쓰지 않아도 된다.
  - 학습률(learning rate)를 크게 잡아도 gradient descent가 잘 수렴한다.
  - 오버피팅을 억제한다. BN이 마치 Regularization 역할을 하기 때문에 드롭아웃(Dropout)과 같은 규제기법에 대한 필요성이 감소한다.  하지만, BN로 인한 규제는 효과가 크지 않기 때문에 드롭아웃을 함께 사용하는 것이 좋다.
  - batch 단위(vector내의 element가 아닌, vector간의 같은 위치의 element에 대해 평균을 냄)
  - 다음의 경우에 잘 작동하지 않는다.
    - 개별 데이터가 커서, mini-batch의 샘플 수가 너무 적은 경우
    - RNN같은 sequence 데이터의 경우에 각각의 Time-step마다 BN을 적용시켜야 하는데 이는 엄청난 계산량이 든다.
    - 가장 중요한 것은 각각 time-step마다 다른 BN이 학습된다.

- Layer Normalization 
  - BN과 달리 각층 내부의 정규화를 시행한다. (한 층의 layer들의 hidden variable H에 대해서 정규화 수행)
  - 문자열을 다루는 문제(transformer)와 같이 seq가 중요한 모델은 이 정규화 모델 사용.

#######Reference
http://igl-blog.com/tag/normalization/
https://wingnim.tistory.com/92
